[{"id":0,"href":"/showcase/docs/Talleres/Actividades/","title":"Actividades","section":"Talleres","content":" Actividades # En este espacio se mostraran algunas de las actividades desarrolladas en clase.\nSolidos Platonicos Sólidos Platónicos # Con el proposito de familiarizarse con el uso de la plantilla y P5.js, se implementa el sólido platónico Octaedro, logrando el siguiente resultado. "},{"id":1,"href":"/showcase/docs/Talleres/Actividades/Solidos_Platonicos/","title":"Solidos Platonicos","section":"Actividades","content":" Sólidos Platónicos # Con el proposito de familiarizarse con el uso de la plantilla y P5.js, se implementa el sólido platónico Octaedro, logrando el siguiente resultado.\n"},{"id":2,"href":"/showcase/docs/Talleres/Ilusiones/","title":"Ilusiones","section":"Talleres","content":" Ilusiones visuales # El desarrollo de este taller tiene como objetivo implementar la ilusion visual \u0026ldquo;Stepping Feet\u0026rdquo; e implementar una aplicación visual de procesamiento de imágenes en un software que tenga diferentes núcleos de imágenes y visualización de histogramas de imágenes.\nEl desarrollo del taller corresponde a los temas:\nFenomenos visuales e ilusiones opticas Masking Masking Masking - Image Kernel # Introducción # Image Processing o procesamiento digital de imagenes, es el producto de utilizar un kernel, matriz de convolucion o mascara, para el desenfoque, enfoque, realce, detección de bordes, entre otros. Contexto # El proceso de aplicar una mascara sobre una imagen, consta en modificar el valor de cada pixel de la imagen, tomando informacion de los pixeles que la rodean, dependiendo del kernel se da prioridad a ciertos pixeles sobre otro, se pueden lograr efectos diferentes sobre la imagen modificando el kernel. Stepping Feet \u0026ldquo;Stepping Feet\u0026rdquo; Motion Illusion # Introducción # Stepping Feet o ilusion de los pies escalonadoes es un fenomeno de percepcion de movimiento en el cual se observa el movimiento de dos bloques un azul y otro amarillo. Los bloques parecen caminar alternativamente. El movimiento es más pronunciado si no se miran directamente los bloques, sino entre ellos. Aunque parecen ser pies dando pasos, en realidad su movimiento es siempre simultáneo. "},{"id":3,"href":"/showcase/docs/Talleres/Ilusiones/Masking/","title":"Masking","section":"Ilusiones","content":" Masking - Image Kernel # Introducción # Image Processing o procesamiento digital de imagenes, es el producto de utilizar un kernel, matriz de convolucion o mascara, para el desenfoque, enfoque, realce, detección de bordes, entre otros.\nContexto # El proceso de aplicar una mascara sobre una imagen, consta en modificar el valor de cada pixel de la imagen, tomando informacion de los pixeles que la rodean, dependiendo del kernel se da prioridad a ciertos pixeles sobre otro, se pueden lograr efectos diferentes sobre la imagen modificando el kernel.\nResultados y Código (Solución) # Image Processing - Instrucciones Instrucciones de uso\nPresione la tecla \u0026lsquo;c\u0026rsquo; para intercambiar entre la imagen original y la modificada. Presione la tecla \u0026lsquo;i\u0026rsquo; para intercambiar entre la vista de configuracion y la vista de visualizacion de la imagen Presione la tecla \u0026rsquo;s\u0026rsquo; para redimensionar la imagen Discusión # Diferentes resultados se obtienen con diferentes kernels. Dependiendo de la imagen se pueden obtener mejores resultados. Conclusiones # El campo de Image Processing tiene varias aplicaciones, una de las mas conocidas es utilizando un kernel Gaussian, el cual difumina la imagen original.\nReferencias # Wikipedia. Kernel (image processing). https://en.wikipedia.org/wiki/Kernel_%28image_processing%29\nJean Pierre Charalambos. p5.quadrille.js. https://objetos.github.io/p5.quadrille.js/\n"},{"id":4,"href":"/showcase/docs/Talleres/Ilusiones/Stepping_Feet/","title":"Stepping Feet","section":"Ilusiones","content":" \u0026ldquo;Stepping Feet\u0026rdquo; Motion Illusion # Introducción # Stepping Feet o ilusion de los pies escalonadoes es un fenomeno de percepcion de movimiento en el cual se observa el movimiento de dos bloques un azul y otro amarillo. Los bloques parecen caminar alternativamente. El movimiento es más pronunciado si no se miran directamente los bloques, sino entre ellos. Aunque parecen ser pies dando pasos, en realidad su movimiento es siempre simultáneo.\nContexto # Stuart Anstis demostró por primera vez esta ilusión en 2003.\nEn la ilusión cuando el bloque azul se encuentra sobre las franjas blancas, el contraste es alto (azul oscuro frente a blanco) y fácilmente visible, por lo que parece moverse más rápido que su velocidad real. Por el contrario, cuando el bloque azul está contra las franjas negras, el contraste es bajo (azul oscuro vs. negro) y más difícil de ver, por lo que el movimiento parece más lento. Los efectos opuestos ocurren para el bloque amarillo.\nResultados y Código (Solución) # Presione la tecla \u0026lsquo;0\u0026rsquo; para quitar la textura. Mueva la barra deslizante para aumentar o disminuir la velocidad. stepping-feet let x = 0; let xspeed = 1; let slider; let flag = true; let val; function setup() { createCanvas(710, 310); noStroke(); slider = createSlider(0, 3, 1, 0.5); slider.position(5, 320); } function draw() { background(0); val = slider.value(); if (flag) { xspeed = val; } else { xspeed = -val; } // Quitar la textura if (keyIsPressed \u0026amp;\u0026amp; key == \u0026#39;0\u0026#39;) { background(150); } else { createBars(); } moveBrick(); } // Función para crear las franjas verticales blancas function createBars() { let len = 12; for (let i = 0; i \u0026lt; width / len; i++) { fill(255); if (i % 2 == 0) rect(i * len, height, len, -height); } } // Funcion para mover los bloques function moveBrick() { fill(0, 0, 255); rect(x, 100, 70, 30); fill(255, 255, 0); rect(x, 200, 70, 30); if (x + 70 \u0026gt;= width) { flag = false; } else if (x \u0026lt;= 0) { flag = true; } x += xspeed; } Discusión # En general, los movimientos de mayor contraste se ven más rápidos que los de menor contraste. El efecto desaparece cuando se quita la textura ya que no queda contraste, mostrando cómo el fondo de un objeto puede tener un efecto significativo en su velocidad percibida. Conclusiones # Al trabajar en esta ilusion se logra estudiar el efecto contraste, el cual es la tendencia añadir o reducir el valor de los objetos que percibimos al compararlos con otro objeto. Evidenciando que el contraste no solo modifica la latencia para esta ilusión, sino también la amplitud del movimiento percibido.\nReferencias # Bach, M. (s. f.). “Stepping feet” Motion Illusion. 148 Visual Phenomena \u0026amp; Optical Illusions. https://michaelbach.de/ot/mot-feetLin/\nAnstis, S (2003). \u0026ldquo;Moving Objects Appear to Slow Down at Low Contrasts\u0026rdquo;. Neural Networks. 16 (5): 933–938. doi:10.1016/S0893-6080(03)00111-4.\nWikipedia. Stepping feet illusion. https://en.wikipedia.org/wiki/Stepping_feet_illusion, Contrast effect https://en.wikipedia.org/wiki/Contrast_effect\n"},{"id":5,"href":"/showcase/docs/Talleres/Rendering/Anti-aliasing/","title":"Anti Aliasing","section":"Rendering","content":" Anti-aliasing # Introducción # Con el siguinte taller se busca revisar los conceptos de coordenadas baricentricas, rasterización y aliasing al realizar una implementación del suavizado o antiescalinamiento(anti-aliasing) en p5.\nContexto # El antialiasing es una técnica utilizada en gráficos por computadora para eliminar el efecto de aliasing. Este efecto consiste en la aparición de bordes irregulares o \u0026ldquo;jaggies\u0026rdquo; en una imagen rasterizada (una imagen renderizada usando píxeles). El problema de los bordes irregulares técnicamente ocurre debido a la distorsión de la imagen cuando la conversión del escaneo se realiza con muestreo a baja frecuencia, el cual resulta en la pérdida de información de la imagen.\nEl proceso de antialiasing determina qué color debemos usar cuando rellenamos píxeles. Como se puede observar en la siguente imagen, cuando el antialiasing está habilitado, los píxeles son tonos de gris, sin embargo, cuando se deshabilita, el píxel se rellena como negro o blanco sólido y la forma se ve irregular.\nMuestreo múltiple Antialiasing (MSAA - Multisampling) # Existen diferente metodos para hacer que la imagen sea más agradable para el usuario, entre ellos se encuentra el MSAA - Multisampling, este metodo es uno de los más basicos sin embargo es uno de los más usados en juegos de rango medio, ya que tiene un equilibrio entre rendimiento y calidad.\nEl antialiasing MSAA se realiza siguiendo los siguientes pasos:\nSe toma una imagen que tiene jaggies. La imagen se representa en su estructura de alta resolución. A alta resolución, se toman muestras de color de píxeles adicionales que estaban ausentes en la imagen de baja resolución. A baja resolución, cada píxel obtiene otro color al que se ha llegado a un promedio de píxeles adicionales. El nuevo color ayuda a que los píxeles se mezclen de manera más efectiva y los jaggies resultan ser menos notables. Para entrar en más detalle de como funciona este método primero se deben entender el concepto de rasterización el cual toma todos los vértices que pertenecen a una objeto y los transforma en un conjunto de fragmentos. Para la implementación realizada en este taller se hace uso del calculo de coordenadas baricentricas.\nA continuación se muestra una cuadrícula de píxeles de pantalla, donde el centro de cada píxel contiene un punto de muestreo que se utiliza para determinar si un píxel está cubierto o no por el triángulo. Los puntos de muestra rojos indican que están cubiertos por el triángulo y para ellos se generia un fragmento, dando como resultado una imagen con bordes irregulares.\nAl hacer uso del método MSAA, este no usa un único punto de muestreo para determinar la cobertura del triángulo, sino múltiples puntos de muestreo. Por ejemplo en la siguinete imagen se tienen 4 submuestras en un patrón general y estos se utilizan para determinar la cobertura de los píxeles. El lado izquierdo de la imagen se muestra cómo determinaríamos normalmente la cobertura de un triángulo. Este píxel específico no ejecutará un sombreador de fragmentos (y, por lo tanto, permanecerá en blanco) ya que su punto de muestra no estaba cubierto por el triángulo. El lado derecho de la imagen muestra una versión multimuestreada donde cada píxel contiene 4 puntos de muestra. Aquí podemos ver que solo 2 puntos de muestra cubren el triángulo por lo que obtine un tono del color original, para detemimar el tono a usar en ese pixel se promedian los colores de cada submuestra por píxel.\nEn general el MSAA utiliza un búfer de profundidad/plantilla más grande para determinar la cobertura de la submuestra y el número de submuestras cubiertas determina cuánto contribuye el color del píxel al búfer de fotogramas.\nLa cantidad de puntos de muestra puede ser cualquier número que deseemos, entre más muestras se logra una mejor precisión de cobertura. Continuando con un píxel que contiene 4 submuestras, para cada píxel cuantas menos submuestras forman parte del triángulo, menos toma el color del triángulo. Si tuviéramos que completar los colores reales de los píxeles, obtendríamos la siguiente imagen, donde los bordes irregulares del triángulo ahora están rodeados por colores ligeramente más claros que el color del borde real, lo que hace que el borde parezca suave cuando se ve desde la distancia.\nResultados y Código (Solución) # Presione la tecla \u0026lsquo;1\u0026rsquo;, \u0026lsquo;2\u0026rsquo; o \u0026lsquo;3\u0026rsquo; para mover alguno de los puntos del triangulo y cuaalquier otra tecla para dejar de mover el punto. Discusión y Conclusiones # La técnica del anti-alising es la razón por la que tenemos texto claro y formas vectoriales suaves en nuestras pantallas, esta técnica mejora la precisión de la rasterización al incluir zonas que estén parcialmente cubiertas por el triángulo, dando una mayor resolución y definición de objetos.\nEl método Multisampling es utilizando en equipos de gama media y alta ya que este corrige los jaggies en el polígono y requiere menos potencia de procesamiento por lo que es muy famoso entre los videojuegos.\nReferencias # The barycentric conspiracy. (2017, 9 abril). The Ryg Blog. https://fgiesen.wordpress.com/2013/02/06/the-barycentric-conspirac/ Anti Aliasing. Learn OpenGL. https://learnopengl.com/Advanced-OpenGL/Anti-Aliasing S. (2015, 25 enero). Rasterization: a Practical Implementation (Rasterization: a Practical Implementation). © 2009–2016 Scratchapixel. https://www.scratchapixel.com/lessons/3d-basic-rendering/rasterization-practical-implementation/rasterization-practical-implementation GeeksforGeeks. (2019, 31 enero). Computer Graphics | Antialiasing. https://www.geeksforgeeks.org/computer-graphics-antialiasing/ Wikipedia. (2021). Antialiasing. Wikipedia, la enciclopedia libre. https://es.wikipedia.org/wiki/Antialiasing "},{"id":6,"href":"/showcase/docs/Talleres/Scene-Trees/Main_Spaces/","title":"Main Spaces","section":"Scene Trees","content":" Main Spaces # Introducción # El desarrollo de este taller tiene como objetivo implementar una herramienta de dibujo 3D, apoyandonos en el uso de librerias tales como p5.treegl, p5.EasyCam y ml5.js\nContexto # Teniendo como punto de partida el codigo que el profesor nos brindo en la pagina del curso e integrandolo con una libreria como ml5.js, desarrollamos una herramienta la cual permite dibujar en una interfaz 3D, adicionalmente es capaz de leer ciertos gestos con la mano, dependiendo del gesto detectado se puede cambiar ciertas configuraciones a la hora de dibujar\nResultados y Código (Solución) # Presione la tecla \u0026lsquo;r\u0026rsquo; para grabar. Presione la tecla \u0026lsquo;c\u0026rsquo; para borrar el canvas Presione la tecla \u0026lsquo;p\u0026rsquo; para cambiar el tipo de camara Presione dos veces sobre el canvas para reiniciar la posición de la camara Con la mano haga el gesto \u0026lsquo;☝️\u0026rsquo; para dibujar con una esfera Con la mano haga el gesto \u0026lsquo;✌️\u0026rsquo; para dibujar con una caja Con la mano haga el gesto \u0026lsquo;👌\u0026rsquo; para dibujar con un cono Con la mano haga el gesto \u0026lsquo;✊\u0026rsquo; para cambiar el color main-spaces // Goal in the 3d Brush is double, to implement: // 1. a gesture parser to deal with depth, i.e., // replace the depth slider with something really // meaningful. You may use a 3d sensor hardware // such as: https://en.wikipedia.org/wiki/Leap_Motion // or machine learning software to parse hand (or // body) gestures from a (video) / image, such as: // https://ml5js.org/ // 2. other brushes to stylize the 3d brush, taking // into account its shape and alpha channel, gesture // speed, etc. // Brush controls let depth; let brush; let easycam; let state; let escorzo; let points; let record; let handpose; let video; let hands = []; let colorIndex = 0; let colorsDefault = [\u0026#34;#00ff00\u0026#34;, \u0026#34;#bb00ff\u0026#34;, \u0026#34;#0000ff\u0026#34;, \u0026#34;#ffff00\u0026#34;]; let selectorsColor = []; let selectorFigure; let sizeX = 735 let sizeY = 600 let stateHand = \u0026#34;initial\u0026#34; //Closed, Unknow, Open, One, Two, Three function setup() { createCanvas(sizeX, sizeY, WEBGL); video = createCapture(VIDEO); video.size(sizeX/5, sizeY/5); handpose = ml5.handpose(video, modelReady); // This sets up an event that fills the global variable \u0026#34;predictions\u0026#34; // with an array every time new hand poses are detected // Hide the video element, and just show the canvas video.hide(); handpose.on(\u0026#34;hand\u0026#34;, results =\u0026gt; { hands = results; }); // easycam stuff let state = { distance: 250, // scalar center: [0, 0, 0], // vector rotation: [0, 0, 0, 1], // quaternion }; easycam = createEasyCam(); easycam.state_reset = state; // state to use on reset (double-click/tap) easycam.setState(state, 2000); // now animate to that state escorzo = true; perspective(); // brush stuff points = []; selectorFigure = createSelect(); selectorFigure.position(sizeX-80, 230); selectorFigure.option(\u0026#39;Sphere\u0026#39;); selectorFigure.option(\u0026#39;Box\u0026#39;); selectorFigure.option(\u0026#39;Cone\u0026#39;); selectorFigure.selected(\u0026#39;Sphere\u0026#39;); for (let i = 0; i\u0026lt;4 ; i++){ selectorsColor[i] = createColorPicker(colorsDefault[i]); selectorsColor[i].position(width - 70, 30+(i*50)); } // select initial brush brush = brushFun; } function modelReady() { console.log(\u0026#34;Model ready!\u0026#34;); } //Funcion que mapea una coordenada, normalizando y escalando function mapCord(cord, originMin, originMax, min, max) { return (cord-originMin)/(originMax-originMin) * (max-min) + min } function draw() { update(); background(120); push(); strokeWeight(0.8); stroke(\u0026#39;magenta\u0026#39;); grid({ dotted: false }); pop(); axes(); for (const point of points) { push(); translate(point.worldPosition); brush(point); pop(); } beginHUD(); image(video, 0, 0, sizeX/4, sizeY/4); // fill(colorSel); // noStroke(); // rect(width - 70, 20, 40, 40); noStroke(); fill(selectorsColor[colorIndex].color()); switch (selectorFigure.value()) { case \u0026#34;Sphere\u0026#34;: circle(width - 95, 35+(colorIndex*50), 20); break; case \u0026#34;Box\u0026#34;: square(width - 105, 25+(colorIndex*50), 19); break; case \u0026#34;Cone\u0026#34;: triangle(width - 105, 45+(colorIndex*50), width - 95, 25+(colorIndex*50), width - 85,45+(colorIndex*50)); break; default: break; } drawKeypoints(); endHUD(); } function checkGesture(landmarks){ let fingers = [ landmarks.indexFinger, landmarks.middleFinger, landmarks.pinky, landmarks.ringFinger ] let isOpen = true; let isClosed = true; let isOne = true; let isTwo = true; let isThree = true; for (let finger of fingers){ for(let pos=1; pos \u0026lt; finger.length ; pos++){ if(finger[pos-1][1] \u0026lt;= finger[pos][1]){ isOpen=false; break; } } } if(isOpen) return \u0026#34;Open\u0026#34; for (let finger of fingers){ if(finger[0][1] \u0026gt; finger[3][1]){ isClosed=false; break; } } if(isClosed) return \u0026#34;Closed\u0026#34; fingers = [ landmarks.middleFinger, landmarks.pinky, landmarks.ringFinger ] for (let finger of fingers){ if(finger[0][1] \u0026gt; finger[3][1]){ isOne=false; break; } } for(let pos=1; pos \u0026lt; landmarks.indexFinger.length ; pos++){ if(landmarks.indexFinger[pos-1][1] \u0026lt;= landmarks.indexFinger[pos][1]){ isOne=false; break; } } if(isOne) return \u0026#34;One\u0026#34; fingers = [ landmarks.pinky, landmarks.ringFinger ] for (let finger of fingers){ if(finger[0][1] \u0026gt; finger[3][1]){ isTwo=false; break; } } fingers = [ landmarks.indexFinger, landmarks.middleFinger, ] for (let finger of fingers){ for(let pos=1; pos \u0026lt; finger.length ; pos++){ if(finger[pos-1][1] \u0026lt;= finger[pos][1]){ isTwo=false; break; } } } if(isTwo) return \u0026#34;Two\u0026#34; if(landmarks.indexFinger[2][1] \u0026gt; landmarks.indexFinger[3][1]){ isThree=false; } fingers = [ landmarks.middleFinger, landmarks.pinky, landmarks.ringFinger ] for (let finger of fingers){ for(let pos=1; pos \u0026lt; finger.length ; pos++){ if(finger[pos-1][1] \u0026lt;= finger[pos][1]){ isThree=false; break; } } } if(isThree) return \u0026#34;Three\u0026#34; return \u0026#34;Unknown\u0026#34;; } //Cambia el color cada que se llama (Verde, Morado, Azul, Amarillo, Cian) function switchSelector(){ colorIndex=(colorIndex+1)%4; } function update() { let dx = abs(mouseX - pmouseX); let dy = abs(mouseY - pmouseY); speed = constrain((dx + dy) / (2 * (width - height)), 0, 1); let lastStateHand = stateHand; // console.log(\u0026#34;mouseX\u0026#34; + mouseX + \u0026#34;mouseX\u0026#34; + mouseY); // console.log(\u0026#34;pmouseX\u0026#34; + pmouseX + \u0026#34;pmouseY\u0026#34; + pmouseY); // console.log(\u0026#34;---------\u0026#34;); if(hands[0]?.annotations){ stateHand = checkGesture(hands[0].annotations); console.log(stateHand) if(lastStateHand != stateHand){ if(stateHand == \u0026#34;Closed\u0026#34;){ switchSelector(); } if(stateHand == \u0026#34;One\u0026#34;){ selectorFigure.selected(\u0026#39;Sphere\u0026#39;); } if(stateHand == \u0026#34;Two\u0026#34;){ selectorFigure.selected(\u0026#39;Box\u0026#39;); } if(stateHand == \u0026#34;Three\u0026#34;){ selectorFigure.selected(\u0026#39;Cone\u0026#39;); } } if (record) { points.push({ worldPosition: treeLocation([ mapCord(600-hands[0].annotations.indexFinger[3][0], 30, 600, 0, sizeX), mapCord(hands[0].annotations.indexFinger[3][1], 30, 460, 0, sizeY), mapCord(1-hands[0].annotations.indexFinger[3][2], -60, 60, 0.2, 0.8)], { from: \u0026#39;SCREEN\u0026#39;, to: \u0026#39;WORLD\u0026#39; }), color: selectorsColor[colorIndex].color(), speed: speed, figure: selectorFigure.value(), }); } }else{ stateHand=\u0026#34;Unknown\u0026#34;; } } function brushFun(point) { push(); noStroke(); // TODO parameterize sphere radius and / or // alpha channel according to gesture speed fill(point.color); switch (point.figure) { case \u0026#34;Sphere\u0026#34;: sphere(3); break; case \u0026#34;Box\u0026#34;: box(5); break; case \u0026#34;Cone\u0026#34;: cone(5); break; default: sphere(3); break; } pop(); } function keyPressed() { if (key === \u0026#39;r\u0026#39;) { record = !record; } if (key === \u0026#39;p\u0026#39;) { escorzo = !escorzo; escorzo ? perspective() : ortho(); } if (key == \u0026#39;c\u0026#39;) { points = []; } } function mouseWheel(event) { //comment to enable page scrolling return false; } // A function to draw ellipses over the detected keypoints function drawKeypoints() { for (let i = 0; i \u0026lt; hands.length; i += 1) { const hand = hands[i]; for (let j = 0; j \u0026lt; hand.landmarks.length; j += 1) { const keypoint = hand.landmarks[j]; fill(selectorsColor[colorIndex].color()); noStroke(); ellipse( mapCord(600-keypoint[0], 30, 600, 0, sizeX), mapCord(keypoint[1], 30, 460, 0, sizeY), 10, 10); } } } Discusión # La libreria ml5.js no es precisa a la hora de detectar los movimientos de la mano, la profundidad se altera con facilidad Se pueden programar mas gestos y dar una mayor cantidad de opciones de lo que puede llegar a realizar con solo gestos de la mano Con el uso de un hardware especializado como el LeapMotion se puede llegar a tener un mejor rastreo de los movimientos de las manos, lastimosamente el software del LeapMotion esta desactualizado y sin soporte, las librerias actualmente utilizan un puerto websocket por el cual recibian la informacion del LeapMotion, pero actualmente el controlador ya no crea el servidor por el cual se conecta con las librerias, dejando las libreerias desactualizadas y sin funcionamiento Conclusiones # Se pueden realizar interfaces 3D capaces de detectar gestos para dibujar, con algunas mejoras en el hardware implementado se puede mejorar en la experiencia que tiene el usuario para dibujar\nReferencias # ml5.js Handpose. https://learn.ml5js.org/#/reference/handpose "},{"id":7,"href":"/showcase/docs/Talleres/Shaders/Ejercicios/","title":"Ejercicios","section":"Shaders","content":" Procedural texturing # Introducción # El desarrollo de este taller tiene como objetivo familiarizarse con el uso de shaders\nContexto # En este ejercicio se busca implementar otras herramientas de brillo de color ( coloring brightness ), tales como el valor V de HSV, la luminosidad L de HSL y el promedio de componentes, teniendo como punto de partida el codigo brindado por el profesor en la pagina del curso.\nLos metodos implementados corresponden a las siguientes ecuaciones:\nComponent average \\[ I = avg(R, G, G) = \\frac 1 3 (R \u0026#43; G \u0026#43; B) \\] HSV value V \\[ V = max(R, G, G) = M \\] HSL lightness L \\[ V = mimd(R, G, G) = \\frac 1 2 (M \u0026#43; m) \\] Resultados y Código (Solución) # coloring brightness let lumaShader; let img; let grey_scale; function preload() { lumaShader = readShader(\u0026#39;/showcase/sketches/texturing/brightness.frag\u0026#39;, { varyings: Tree.texcoords2 }); // image source: https://en.wikipedia.org/wiki/HSL_and_HSV#/media/File:Fire_breathing_2_Luc_Viatour.jpg img = loadImage(\u0026#39;/showcase/sketches/fire_breathing.jpg\u0026#39;); } function setup() { createCanvas(700, 500, WEBGL); noStroke(); textureMode(NORMAL); shader(lumaShader); grey_scale = createRadio(); grey_scale.option(\u0026#39;1\u0026#39;, \u0026#39;Color photograph (RGB)\u0026#39;); grey_scale.option(\u0026#39;2\u0026#39;, \u0026#39;Luma\u0026#39;); grey_scale.option(\u0026#39;3\u0026#39;, \u0026#39;Component average\u0026#39;); grey_scale.option(\u0026#39;4\u0026#39;, \u0026#39;HSV\u0026#39;); grey_scale.option(\u0026#39;5\u0026#39;, \u0026#39;HSL\u0026#39;); grey_scale.selected(\u0026#39;1\u0026#39;); grey_scale.changed(() =\u0026gt; lumaShader.setUniform(\u0026#39;grey_scale\u0026#39;, grey_scale.value())); lumaShader.setUniform(\u0026#39;texture\u0026#39;, img); lumaShader.setUniform(\u0026#39;grey_scale\u0026#39;, grey_scale.value()); } function draw() { background(0); quad(-width / 2, -height / 2, width / 2, -height / 2, width / 2, height / 2, -width / 2, height / 2); } brightness.frag precision mediump float; // uniforms are defined and sent by the sketch uniform int grey_scale; uniform sampler2D texture; // interpolated texcoord (same name and type as in vertex shader) varying vec2 texcoords2; // returns luma of given texel float luma(vec3 texel) { return 0.299 * texel.r + 0.587 * texel.g + 0.114 * texel.b; } // returns component average of given texel float average(vec3 texel) { return (texel.r + texel.g + texel.b)/3.0; } // returns HSV value of given texel float hsv(vec3 texel) { return max (max (texel.r, texel.g), texel.b); } // returns HSL lightness of given texel float hsl(vec3 texel) { return max(max(texel.r, texel.g), texel.b) / 2.0 + min(min(texel.r, texel.g), texel.b) / 2.0; } void main() { // texture2D(texture, texcoords2) samples texture at texcoords2 // and returns the normalized texel color vec4 texel = texture2D(texture, texcoords2); // gl_FragColor = grey_scale ? vec4((vec3(luma(texel.rgb))), 1.0) : texel; if(grey_scale == 1) { gl_FragColor = texel; }else if(grey_scale == 2) { gl_FragColor = vec4((vec3(luma(texel.rgb))), 1.0); }else if(grey_scale == 3) { gl_FragColor = vec4((vec3(average(texel.rgb))), 1.0); }else if(grey_scale == 4) { gl_FragColor = vec4((vec3(hsv(texel.rgb))), 1.0); }else{ gl_FragColor = vec4((vec3(hsl(texel.rgb))), 1.0); // } } "},{"id":8,"href":"/showcase/docs/Talleres/Shaders/Image-Processing/","title":"Image Processing","section":"Shaders","content":" Image Processing # Contexto # Aplicar sobre una zona el kernel seleccionado, o aplicar un kernel personalizado sobre una imagen o un video\nResultados y Código (Solución) # Deslice el mouse sobre el canvas para aplicar el shader en la zona. En la esquina superior derecha puede ajustar el kernel. En la esquina superior derecha puede seleccionar entre la imagen y el video. imgprocess.js let image; let video; let videoCheck; let maskOption; let shaderMask; let mask = [0, -1, 0, -1, 5, -1, 0, -1, 0] let input = [9]; let button; let selectorFigure; function preload() { video = createVideo([\u0026#39;/showcase/sketches/imgprocess/dogs.webm\u0026#39;]); video.hide(); // by default video shows up in separate dom shaderMask = loadShader(\u0026#39;/showcase/sketches/imgprocess/shader.vert\u0026#39;, \u0026#39;/showcase/sketches/imgprocess/mask.frag\u0026#39;); image = loadImage(\u0026#39;/showcase/sketches/imgprocess/test.jpg\u0026#39;); // image = loadImage(\u0026#39;/showcase/sketches/imgprocess/test2.jpg\u0026#39;); // image = loadImage(\u0026#39;/showcase/sketches/imgprocess/test3.jpeg\u0026#39;); // image = loadImage(\u0026#39;/showcase/sketches/imgprocess/test4.jpg\u0026#39;); } function setup() { // shaders require WEBGL mode to work createCanvas(700, 550, WEBGL); noStroke(); textureMode(NORMAL); videoCheck = createCheckbox(\u0026#39;video\u0026#39;, false); videoCheck.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); videoCheck .changed(() =\u0026gt; { if (videoCheck.checked()) { shaderMask.setUniform(\u0026#39;texture\u0026#39;, video); video.loop(); } else { shaderMask.setUniform(\u0026#39;texture\u0026#39;, image); video.pause(); } }); videoCheck.position(640, 40); maskOption = createCheckbox(\u0026#39;options\u0026#39;, false); maskOption.position(640, 20); maskOption.style(\u0026#39;color\u0026#39;, \u0026#39;white\u0026#39;); maskOption.changed(() =\u0026gt; { if (maskOption.checked()) { for (const inp in input) { input[inp].show(); selectorFigure.show(); button.show(); } }else{ for (const inp in input) { input[inp].hide(); button.hide(); selectorFigure.hide(); } } }); shader(shaderMask); shaderMask.setUniform(\u0026#39;texture\u0026#39;, image); emitTexOffset(shaderMask, image, \u0026#39;texOffset\u0026#39;); shaderMask.setUniform(\u0026#39;radius\u0026#39;, 50*pixelDensity()); shaderMask.setUniform(\u0026#39;u_resolution\u0026#39;, [width*pixelDensity(), height*pixelDensity()]); for (let i = 0; i \u0026lt; 3; i++) { for (let j = 0; j \u0026lt; 3; j++) { input[i*3+j] = createInput(); input[i*3+j].position(200+(j*50), 200+(i*50)); input[i*3+j].style(\u0026#39;text-align:center\u0026#39;); input[i*3+j].style(\u0026#39;font-size:16px\u0026#39;); input[i*3+j].size(42,44); input[i*3+j].hide(); input[i*3+j].value(mask[i*3+j]); } } button = createButton(\u0026#39;Aplicar\u0026#39;); button.position(400, 265); button.mousePressed(apply); button.hide(); selectorFigure = createSelect(); selectorFigure.position(400, 230); selectorFigure.option(\u0026#39;Sharpen\u0026#39;); selectorFigure.option(\u0026#39;Ridge detection\u0026#39;); selectorFigure.option(\u0026#39;Blur\u0026#39;); selectorFigure.option(\u0026#39;Personalized\u0026#39;); selectorFigure.option(\u0026#39;Identity\u0026#39;); selectorFigure.selected(\u0026#39;Sharpen\u0026#39;); selectorFigure.hide(); selectorFigure.changed(() =\u0026gt; { switch(selectorFigure.value()){ case \u0026#34;Sharpen\u0026#34;: changeInputs([0, -1, 0, -1, 5, -1, 0, -1, 0]); break; case \u0026#34;Ridge detection\u0026#34;: changeInputs([-1, -1, -1, -1, 8, -1, -1, -1, -1]) break; case \u0026#34;Blur\u0026#34;: changeInputs([1/9, 1/9, 1/9, 1/9, 1/9, 1/9, 1/9, 1/9, 1/9]); break; case \u0026#34;Identity\u0026#34;: changeInputs([0, 0, 0, 0, 1, 0, 0, 0, 0]); break; case \u0026#34;Personalized\u0026#34;: apply(); break; } }); } function changeInputs(masks){ for (let i = 0; i \u0026lt; 9; i++) { input[i].value(masks[i]); } apply(); } function apply() { mask = [parseFloat(input[0].value()), parseFloat(input[1].value()), parseFloat(input[2].value()), parseFloat(input[3].value()), parseFloat(input[4].value()), parseFloat(input[5].value()), parseFloat(input[6].value()), parseFloat(input[7].value()), parseFloat(input[8].value())] } function reset() { mask = createQuadrille([[0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625]]); } function draw() { if(maskOption.checked()){ background(\u0026#39;#060621\u0026#39;); }else{ background(0); shaderMask.setUniform(\u0026#39;mask\u0026#39;, mask); shaderMask.setUniform(\u0026#39;u_mouse\u0026#39;, [mouseX*pixelDensity(), (height - mouseY)*pixelDensity()]); cover(true); quad(-width / 2, -height / 2, width / 2, -height / 2, width / 2, height / 2, -width / 2, height / 2); } } function cover(texture = false) { beginShape(); if (texture) { vertex(-width / 2, -height / 2, 0, 0, 0); vertex(width / 2, -height / 2, 0, 1, 0); vertex(width / 2, height / 2, 0, 1, 1); vertex(-width / 2, height / 2, 0, 0, 1); } else { vertex(-width / 2, -height / 2, 0); vertex(width / 2, -height / 2, 0); vertex(width / 2, height / 2, 0); vertex(-width / 2, height / 2, 0); } endShape(CLOSE); } mask.frag precision mediump float; uniform vec2 u_resolution; uniform vec2 u_mouse; uniform float radius; // we need our interpolated color varying vec4 vVertexColor; // we need our interpolated tex coord varying vec2 vTexCoord; uniform sampler2D texture; uniform vec2 texOffset; // holds the 3x3 kernel uniform float mask[9]; void main() { float pct = distance(gl_FragCoord.xy, u_mouse); if (pct \u0026gt;= radius) { gl_FragColor = texture2D(texture, vTexCoord) * vVertexColor; } else { // 1. Use offset to move along texture space. // In this case to find the texcoords of the texel neighbours. vec2 tc0 = vTexCoord + vec2(-texOffset.s, -texOffset.t); vec2 tc1 = vTexCoord + vec2( 0.0, -texOffset.t); vec2 tc2 = vTexCoord + vec2(+texOffset.s, -texOffset.t); vec2 tc3 = vTexCoord + vec2(-texOffset.s, 0.0); // origin (current fragment texcoords) vec2 tc4 = vTexCoord + vec2( 0.0, 0.0); vec2 tc5 = vTexCoord + vec2(+texOffset.s, 0.0); vec2 tc6 = vTexCoord + vec2(-texOffset.s, +texOffset.t); vec2 tc7 = vTexCoord + vec2( 0.0, +texOffset.t); vec2 tc8 = vTexCoord + vec2(+texOffset.s, +texOffset.t); // 2. Sample texel neighbours within the rgba array vec4 rgba[9]; rgba[0] = texture2D(texture, tc0); rgba[1] = texture2D(texture, tc1); rgba[2] = texture2D(texture, tc2); rgba[3] = texture2D(texture, tc3); rgba[4] = texture2D(texture, tc4); rgba[5] = texture2D(texture, tc5); rgba[6] = texture2D(texture, tc6); rgba[7] = texture2D(texture, tc7); rgba[8] = texture2D(texture, tc8); // 3. Apply convolution kernel vec4 convolution; for (int i = 0; i \u0026lt; 9; i++) { convolution += rgba[i]*mask[i]; } // 4. Set color from convolution gl_FragColor = vec4(convolution.rgb, 1.0); } } shader.vert // Precision seems mandatory in webgl precision highp float; // 1. Attributes and uniforms sent by p5.js // Vertex attributes and some uniforms are sent by // p5.js following these naming conventions: // https://github.com/processing/p5.js/blob/main/contributor_docs/webgl_mode_architecture.md // 1.1. Attributes // vertex position attribute attribute vec3 aPosition; // vertex texture coordinate attribute attribute vec2 aTexCoord; // vertex color attribute attribute vec4 aVertexColor; // 1.2. Matrix uniforms // The vertex shader should project the vertex position into clip space: // vertex_clipspace = vertex * projection * view * model (see the gl_Position below) // Details here: http://visualcomputing.github.io/Transformations // Either a perspective or an orthographic projection uniform mat4 uProjectionMatrix; // modelview = view * model uniform mat4 uModelViewMatrix; // B. varying variable names are defined by the shader programmer: // vertex color varying vec4 vVertexColor; // vertex texcoord varying vec2 vTexCoord; void main() { // copy / interpolate color vVertexColor = aVertexColor; // copy / interpolate texcoords vTexCoord = aTexCoord; // vertex projection into clipspace gl_Position = uProjectionMatrix * uModelViewMatrix * vec4(aPosition, 1.0); } Discusión # Utilizando mascaras de convolucion adecuadas se podrian a llegar a detectar objetos, sin el uso de machine learning Conclusiones # El procesamiento en tiempo real de una imagen puede ser muy util sobre todo en el area de la salud El uso de la GPU con la libreria de WEBGL permite que el procesamiento en tiempo real de una imagen o video sea fluidos Referencias # Jean Pierre Charalambos. p5.treegl.js. https://github.com/VisualComputing/p5.treegl Wikipedia. Kernel (image processing). https://en.wikipedia.org/wiki/Kernel_%28image_processing%29 "},{"id":9,"href":"/showcase/docs/Talleres/Shaders/Procedural-texturing/","title":"Procedural Texturing","section":"Shaders","content":" Texture sampling - coloring brightness # Introducción # El desarrollo de este taller tiene como objetivo familiarizarse con el uso de shaders\nContexto # En este ejercicio se busca implementar un algoritmo el cual permita crear un patron el cual pueda ser asignada como textura a una figura.\nA continuación se muestra un patron similar al de una pared de ladrillos donde cada subregión es pintada.\nResultados y Código (Solución) # pattern let pg; let truchetShader; let colorS; function preload() { // shader adapted from here: https://thebookofshaders.com/09/ truchetShader = readShader(\u0026#39;/showcase/sketches/texturing/pattern.frag\u0026#39;, { matrices: Tree.NONE, varyings: Tree.NONE }); } function setup() { createCanvas(400, 400, WEBGL); // create frame buffer object to render the procedural texture pg = createGraphics(400, 400, WEBGL); textureMode(NORMAL); noStroke(); pg.noStroke(); pg.textureMode(NORMAL); // use truchetShader to render onto pg pg.shader(truchetShader); // emitResolution, see: // https://github.com/VisualComputing/p5.treegl#macros pg.emitResolution(truchetShader); // https://p5js.org/reference/#/p5.Shader/setUniform truchetShader.setUniform(\u0026#39;u_zoom\u0026#39;, 3); // pg clip-space quad (i.e., both x and y vertex coordinates ∈ [-1..1]) pg.quad(-1, -1, 1, -1, 1, 1, -1, 1); // set pg as texture texture(pg); } function draw() { background(33); orbitControl(); rotateY(millis() / 800); cone(100, 200); } function mouseMoved() { // https://p5js.org/reference/#/p5.Shader/setUniform truchetShader.setUniform(\u0026#39;u_zoom\u0026#39;, int(map(mouseX, 0, width, 1, 30))); // pg clip-space quad (i.e., both x and y vertex coordinates ∈ [-1..1]) pg.quad(-1, -1, 1, -1, 1, 1, -1, 1); } pattern.frag // Author @patriciogv ( patriciogonzalezvivo.com ) - 2015 #ifdef GL_ES precision mediump float; #endif uniform vec2 u_resolution; uniform float u_time; uniform bool colorSh; vec2 brickTile(vec2 _st, float _zoom){ _st *= _zoom; // Here is where the offset is happening _st.x += step(1., mod(_st.y,2.0)) * 0.5; return fract(_st); } float box(vec2 _st, vec2 _size){ _size = vec2(0.5)-_size*0.5; vec2 uv = smoothstep(_size,_size+vec2(1e-4),_st); uv *= smoothstep(_size,_size+vec2(1e-4),vec2(1.0)-_st); return uv.x*uv.y; } void main(void){ vec2 st = gl_FragCoord.xy/u_resolution.xy; vec3 color = vec3(0.0); // Apply the brick tiling st = brickTile(st,5.0); color = vec3(box(st,vec2(0.9))); color = vec3(st,0.0); gl_FragColor = vec4(color,1.0); } "},{"id":10,"href":"/showcase/docs/Talleres/Shaders/Texturing/","title":"Texturing","section":"Shaders","content":" Texturing # Introducción # El desarrollo de este taller tiene como objetivo familiarizarse con el uso de shaders\nContexto # Se desea aplicar un shader con una figura en especifico definiendo cada vertice de la figura a que posicion (u,v) del shader se debera aplicar\nResultados y Código (Solución) # Deslice el mouse sobre el canvas para aplicar el shader en la zona. Con el Mouse Wheeler puede cambiar el tamaño del shader aplicado. texturing.js let easycam; let uvShader; let opacity; let size; function preload() { // The projection and modelview matrices may be emitted separately // (i.e., matrices: Tree.pMatrix | Tree.mvMatrix), which actually // leads to the same gl_Position result. // Interpolate only texture coordinates (i.e., varyings: Tree.texcoords2). // see: https://github.com/VisualComputing/p5.treegl#handling uvShader = readShader(\u0026#39;../../../../sketches/texturing/uv_alpha.frag\u0026#39;, { varyings: Tree.texcoord2 }); } function setup() { createCanvas(600, 600, WEBGL); size = 2; // easycam stuff let state = { distance: 250, // scalar center: [0, 0, 0], // vector rotation: [0, 0, 0, 1], // quaternion }; easycam = createEasyCam(); easycam.state_reset = state; // state to use on reset (double-click/tap) easycam.setState(state, 2000); // now animate to that state textureMode(NORMAL); opacity = createSlider(0, 1, 0.5, 0.01); opacity.position(10, 25); opacity.style(\u0026#39;width\u0026#39;, \u0026#39;280px\u0026#39;); } function draw() { background(200); // reset shader so that the default shader is used to render the 3D scene resetShader(); // world space scene axes(); grid(); translate(0, -70); rotateY(0.5); fill(color(255, 0, 255, 125)); box(30, 50); translate(70, 70); fill(color(0, 255, 255, 125)); sphere(30, 50); // use custom shader shader(uvShader); // https://p5js.org/reference/#/p5.Shader/setUniform uvShader.setUniform(\u0026#39;opacity\u0026#39;, opacity.value()); // screen-space quad (i.e., x ∈ [0..width] and y ∈ [0..height]) // see: https://github.com/VisualComputing/p5.treegl#heads-up-display beginHUD(); noStroke(); beginShape(); vertex(mouseX, mouseY + (27*size), 10, 1, 1); vertex(mouseX + (25 *size), mouseY + (10*size), 10, 1, 0); vertex(mouseX + (20 *size), mouseY - (20*size), 10, 0, 1); vertex(mouseX - (20 *size), mouseY - (20*size), 10, 0, 1); vertex(mouseX - (25 *size), mouseY + (10*size), 10, 1, 0); endShape(); endHUD(); } function keyPressed() { if (key === \u0026#39;r\u0026#39;) { size = 1; } } function mouseWheel(event) { if(event.delta \u0026gt; 0 ) size += 0.1 else size -= 0.1 return false; } uv_alpha.frag precision mediump float; varying vec2 texcoords2; varying vec4 color4; // uniform is sent by the sketch uniform float opacity; void main() { gl_FragColor = vec4(0.0, texcoords2.yx, opacity); } "}]